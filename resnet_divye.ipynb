{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision import datasets, transforms, models\nfrom torch.utils.data import DataLoader\nimport numpy as np","metadata":{"execution":{"iopub.status.busy":"2024-10-27T11:33:47.788304Z","iopub.execute_input":"2024-10-27T11:33:47.788690Z","iopub.status.idle":"2024-10-27T11:33:53.079877Z","shell.execute_reply.started":"2024-10-27T11:33:47.788650Z","shell.execute_reply":"2024-10-27T11:33:53.079054Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# Device configuration\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Data transformations with Mixup (augmentation) and normalization\ntransform_train = transforms.Compose([\n    transforms.RandomResizedCrop(224),\n    transforms.RandomHorizontalFlip(),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\ntransform_val = transforms.Compose([\n    transforms.Resize(256),\n    transforms.CenterCrop(224),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\n# Load dataset\ntrain_dataset = datasets.ImageFolder(root=\"/kaggle/input/recycle-with-split/recycle_split/train\", transform=transform_train)\nval_dataset = datasets.ImageFolder(root=\"/kaggle/input/recycle-with-split/recycle_split/test\", transform=transform_val)\n\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-27T11:33:53.081454Z","iopub.execute_input":"2024-10-27T11:33:53.081855Z","iopub.status.idle":"2024-10-27T11:34:01.889183Z","shell.execute_reply.started":"2024-10-27T11:33:53.081821Z","shell.execute_reply":"2024-10-27T11:34:01.888396Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torchvision.models as models\n\n# Define the number of classes for your classification task\nnum_classes = 4  # Update this with the actual number of classes\n\n# Load the ResNet model pre-trained on ImageNet\nmodel = models.resnet50(pretrained=True)\n\n# Modify the final fully connected layer to match the number of classes\nmodel.fc = nn.Linear(model.fc.in_features, num_classes)\n\n# Move the model to the GPU if available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)\n\n# Example of defining a label smoothing loss function\ncriterion = nn.CrossEntropyLoss(label_smoothing=0.1)\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-27T11:34:01.890322Z","iopub.execute_input":"2024-10-27T11:34:01.890709Z","iopub.status.idle":"2024-10-27T11:34:03.378933Z","shell.execute_reply.started":"2024-10-27T11:34:01.890664Z","shell.execute_reply":"2024-10-27T11:34:03.377871Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n100%|██████████| 97.8M/97.8M [00:00<00:00, 187MB/s] \n","output_type":"stream"}]},{"cell_type":"code","source":"# Training the model\nnum_epochs = 30\nfor epoch in range(num_epochs):\n    model.train()\n    running_loss = 0.0\n    correct = 0\n    total = 0\n    \n    for images, labels in train_loader:\n        images, labels = images.to(device), labels.to(device)\n        \n        optimizer.zero_grad()\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item() * images.size(0)\n        _, predicted = outputs.max(1)\n        correct += (predicted == labels).sum().item()\n        total += labels.size(0)\n\n    epoch_loss = running_loss / total\n    epoch_acc = 100 * correct / total\n    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.2f}%')\n\n# Validation loop\nmodel.eval()\nval_loss = 0.0\ncorrect = 0\ntotal = 0\nwith torch.no_grad():\n    for images, labels in val_loader:\n        images, labels = images.to(device), labels.to(device)\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n        \n        val_loss += loss.item() * images.size(0)\n        _, predicted = outputs.max(1)\n        correct += (predicted == labels).sum().item()\n        total += labels.size(0)\n\nval_loss /= total\nval_accuracy = 100 * correct / total\nprint(f'Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%')\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-10-27T11:36:12.903260Z","iopub.execute_input":"2024-10-27T11:36:12.903896Z","iopub.status.idle":"2024-10-27T12:39:28.475900Z","shell.execute_reply.started":"2024-10-27T11:36:12.903856Z","shell.execute_reply":"2024-10-27T12:39:28.474885Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Epoch [1/30], Loss: 1.1238, Accuracy: 56.47%\nEpoch [2/30], Loss: 1.0435, Accuracy: 61.76%\nEpoch [3/30], Loss: 0.9946, Accuracy: 64.77%\nEpoch [4/30], Loss: 0.9469, Accuracy: 67.64%\nEpoch [5/30], Loss: 0.9281, Accuracy: 69.01%\nEpoch [6/30], Loss: 0.8960, Accuracy: 70.94%\nEpoch [7/30], Loss: 0.8839, Accuracy: 71.19%\nEpoch [8/30], Loss: 0.8673, Accuracy: 72.46%\nEpoch [9/30], Loss: 0.8648, Accuracy: 72.97%\nEpoch [10/30], Loss: 0.8509, Accuracy: 73.62%\nEpoch [11/30], Loss: 0.8340, Accuracy: 74.80%\nEpoch [12/30], Loss: 0.8216, Accuracy: 75.23%\nEpoch [13/30], Loss: 0.8128, Accuracy: 75.89%\nEpoch [14/30], Loss: 0.8021, Accuracy: 77.10%\nEpoch [15/30], Loss: 0.7956, Accuracy: 76.48%\nEpoch [16/30], Loss: 0.7836, Accuracy: 77.58%\nEpoch [17/30], Loss: 0.7769, Accuracy: 77.73%\nEpoch [18/30], Loss: 0.7737, Accuracy: 77.65%\nEpoch [19/30], Loss: 0.7507, Accuracy: 79.46%\nEpoch [20/30], Loss: 0.7640, Accuracy: 78.72%\nEpoch [21/30], Loss: 0.7540, Accuracy: 79.11%\nEpoch [22/30], Loss: 0.7395, Accuracy: 80.39%\nEpoch [23/30], Loss: 0.7357, Accuracy: 80.37%\nEpoch [24/30], Loss: 0.7318, Accuracy: 80.54%\nEpoch [25/30], Loss: 0.7259, Accuracy: 80.64%\nEpoch [26/30], Loss: 0.7290, Accuracy: 80.76%\nEpoch [27/30], Loss: 0.7083, Accuracy: 81.59%\nEpoch [28/30], Loss: 0.7011, Accuracy: 82.31%\nEpoch [29/30], Loss: 0.6975, Accuracy: 82.18%\nEpoch [30/30], Loss: 0.6933, Accuracy: 82.68%\nValidation Loss: 0.7287, Validation Accuracy: 80.94%\n","output_type":"stream"}]}]}
